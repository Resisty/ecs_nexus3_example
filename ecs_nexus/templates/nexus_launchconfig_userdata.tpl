#!/bin/bash
# install nfs-utils so we can mount EFS and python-pip so we can pip install
sudo yum install -y nfs-utils python-pip
# install aws cli so we can kms decrypt
sudo python-pip install awscli
# create a directory to mount EFS to
sudo mkdir -p ${efs_mountpoint}
# mount the efs volume
sudo mount -t nfs4 -o nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2 ${efs_dnsname}:/ ${efs_mountpoint}
# create fstab entry to ensure automount on reboots
echo "${efs_dnsname}:/ ${efs_mountpoint} nfs defaults,vers=4.1 0 0" >> /etc/fstab
# create the nexus storage and work folders
# hosts share storage, but each host needs its own efs_sonatype_work which the containers all mount as the same name, so use symlinks
az="$(curl http://169.254.169.254/latest/meta-data/placement/availability-zone)"
sudo mkdir -p ${efs_mountpoint}${efs_nexus_storage}
sudo mkdir -p ${efs_mountpoint}${efs_sonatype_work}-$${az}/etc/fabric/
sudo ln -s ${efs_mountpoint}${efs_sonatype_work}-$${az} ${efs_sonatype_work}
# configure clustering discovery with hazelcast
(
sudo cat <<EOM
<?xml version="1.0" encoding="UTF-8"?>
<!--#                                                                        #-->
<!--# WARNING: DO NOT EDIT THIS FILE!                                        #-->
<!--# It is merely a template that will get overwritten during each upgrade. #-->
<!--# Customizations belong in                                               #-->
<!--#   $data-dir/etc/fabric/hazelcast-network.xml                           #-->
<!--#                                                                        #-->
<hazelcast xmlns="http://www.hazelcast.com/schema/config"
           xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
           xsi:schemaLocation="http://www.hazelcast.com/schema/config/hazelcast-config-3.6.xsd">
  <network>
    <public-address>$(hostname | tr '-' '.' | cut -c 4-)</public-address>
    <port auto-increment="true" port-count="100">5701</port>

    <outbound-ports>
      <!--
          Allowed port range when connecting to other nodes.
          0 or * means use system provided port.
      -->
      <ports>0</ports>
    </outbound-ports>

    <join>
      <multicast enabled="false"/>
      <tcp-ip enabled="false"/>
      <aws enabled="true">
        <!-- 
         | Required: use the command 'aws iam list-instance-profiles' on the EC2 instance to locate the name
         | of the role used by the IAM Instance Profile you created previously
         -->
        <iam-role>${ec2_iam_role}</iam-role>
        <!-- Required: set this to the region where your EC2 instances running Nexus Repository Manager are located -->
        <region>${region}</region>
        <!--
         | The next few options give you a choice to how the nodes are enumerated. You can specify:
         | * Just a security group name, if the security group only contains Nexus Repository Manager hosts
         | * or a tag-key/tag-value pair, if you have tagged the EC2 instances
         | * or a combination of the two, if the security group has other hosts in it.
         -->
        <security-group-name>${sg_name}</security-group-name>
        <!-- example tag idea, you are free to use whatever tag naming convention you need-->
        <host-header>ec2.amazonaws.com</host-header>
        <tag-key>${hazelcast_clustering_tag_key}</tag-key>
        <tag-value>${hazelcast_clustering_tag_value}</tag-value>
      </aws>
      <discovery-strategies>
      </discovery-strategies>
    </join>

    <interfaces enabled="false">
      <interface>$(echo "${vpc_cidr}" | awk '{split($1,a,".");split($1,b,"/");print a[1] "." a[2] "." a[3] ".*"}')</interface>
    </interfaces>

    <ssl enabled="false"/>

    <socket-interceptor enabled="false"/>

    <symmetric-encryption enabled="false">
      <!--
         encryption algorithm such as
         DES/ECB/PKCS5Padding,
         PBEWithMD5AndDES,
         AES/CBC/PKCS5Padding,
         Blowfish,
         DESede
      -->
      <algorithm>PBEWithMD5AndDES</algorithm>
      <!-- salt value to use when generating the secret key -->
      <salt>thesalt</salt>
      <!-- pass phrase to use when generating the secret key -->
      <password>thepass</password>
      <!-- iteration count to use when generating the secret key -->
      <iteration-count>19</iteration-count>
    </symmetric-encryption>
  </network>
</hazelcast>
EOM
) > ${efs_mountpoint}${efs_sonatype_work}-$${az}/etc/fabric/hazelcast-network.xml
# create the license file in the efs_sonatype_work
/usr/local/bin/aws kms --region ${region} decrypt --ciphertext-blob fileb://<(echo "very_very_long_encrypted_license_blob_here" | base64 --decode) --output text --query Plaintext | base64 --decode > ${efs_mountpoint}${efs_sonatype_work}-$${az}/etc/sonatype-license.lic
# create the nexus properties file pointing to license file and enable clustering
(
sudo cat <<EOM
# Jetty section
application-port=8081
application-port-ssl=8443
application-host=0.0.0.0
nexus-args=\$${jetty.etc}/jetty.xml,\$${jetty.etc}/jetty-http.xml,\$${jetty.etc}/jetty-https.xml,\$${jetty.etc}/jetty-requestlog.xml
nexus-context-path=/

# Nexus section
nexus-edition=nexus-pro-edition
nexus-features=\
 nexus-pro-feature

 nexus.clustered=true
 # nexus.licenseFile is only necessary for the first run
 # replace /path/to/your/sonatype-license.lic with the path to your license, and ensure the user running Nexus Repository manager can read it
 nexus.licenseFile=/nexus-data/etc/sonatype-license.lic
EOM
) > ${efs_mountpoint}${efs_sonatype_work}-$${az}/etc/nexus.properties
# make sure nexus has access to all files
sudo chown -R 200:200 ${efs_mountpoint}${efs_nexus_storage}
sudo chown -R 200:200 ${efs_mountpoint}${efs_sonatype_work}-$${az}
# join the cluster
echo ECS_CLUSTER=${clustername} >> /etc/ecs/ecs.config
sudo yum update -y ecs-init
# update limits
echo "root - nofile 65536" >> /etc/security/limits.conf
